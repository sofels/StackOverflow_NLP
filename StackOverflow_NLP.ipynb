{"cells":[{"cell_type":"markdown","source":["### Stack Overflow: Using NLP to find answers to top unanswered question"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"19351488-5f19-473d-9c51-bf1f4ecdacbc","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["!pip install nltk"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"036fb4c9-1c00-4ed2-a1f7-012505906e0b","inputWidgets":{},"title":""}},"outputs":[{"output_type":"stream","output_type":"stream","name":"stdout","text":["Collecting nltk\r\n  Downloading nltk-3.8.1-py3-none-any.whl (1.5 MB)\r\n\u001B[?25l\r\u001B[K     |▏                               | 10 kB 17.1 MB/s eta 0:00:01\r\u001B[K     |▍                               | 20 kB 7.6 MB/s eta 0:00:01\r\u001B[K     |▋                               | 30 kB 6.7 MB/s eta 0:00:01\r\u001B[K     |▉                               | 40 kB 5.5 MB/s eta 0:00:01\r\u001B[K     |█                               | 51 kB 4.7 MB/s eta 0:00:01\r\u001B[K     |█▎                              | 61 kB 5.5 MB/s eta 0:00:01\r\u001B[K     |█▌                              | 71 kB 5.6 MB/s eta 0:00:01\r\u001B[K     |█▊                              | 81 kB 6.3 MB/s eta 0:00:01\r\u001B[K     |██                              | 92 kB 6.2 MB/s eta 0:00:01\r\u001B[K     |██▏                             | 102 kB 5.2 MB/s eta 0:00:01\r\u001B[K     |██▍                             | 112 kB 5.2 MB/s eta 0:00:01\r\u001B[K     |██▋                             | 122 kB 5.2 MB/s eta 0:00:01\r\u001B[K     |██▉                             | 133 kB 5.2 MB/s eta 0:00:01\r\u001B[K     |███                             | 143 kB 5.2 MB/s eta 0:00:01\r\u001B[K     |███▎                            | 153 kB 5.2 MB/s eta 0:00:01\r\u001B[K     |███▌                            | 163 kB 5.2 MB/s eta 0:00:01\r\u001B[K     |███▊                            | 174 kB 5.2 MB/s eta 0:00:01\r\u001B[K     |████                            | 184 kB 5.2 MB/s eta 0:00:01\r\u001B[K     |████▏                           | 194 kB 5.2 MB/s eta 0:00:01\r\u001B[K     |████▍                           | 204 kB 5.2 MB/s eta 0:00:01\r\u001B[K     |████▌                           | 215 kB 5.2 MB/s eta 0:00:01\r\u001B[K     |████▊                           | 225 kB 5.2 MB/s eta 0:00:01\r\u001B[K     |█████                           | 235 kB 5.2 MB/s eta 0:00:01\r\u001B[K     |█████▏                          | 245 kB 5.2 MB/s eta 0:00:01\r\u001B[K     |█████▍                          | 256 kB 5.2 MB/s eta 0:00:01\r\u001B[K     |█████▋                          | 266 kB 5.2 MB/s eta 0:00:01\r\u001B[K     |█████▉                          | 276 kB 5.2 MB/s eta 0:00:01\r\u001B[K     |██████                          | 286 kB 5.2 MB/s eta 0:00:01\r\u001B[K     |██████▎                         | 296 kB 5.2 MB/s eta 0:00:01\r\u001B[K     |██████▌                         | 307 kB 5.2 MB/s eta 0:00:01\r\u001B[K     |██████▊                         | 317 kB 5.2 MB/s eta 0:00:01\r\u001B[K     |███████                         | 327 kB 5.2 MB/s eta 0:00:01\r\u001B[K     |███████▏                        | 337 kB 5.2 MB/s eta 0:00:01\r\u001B[K     |███████▍                        | 348 kB 5.2 MB/s eta 0:00:01\r\u001B[K     |███████▋                        | 358 kB 5.2 MB/s eta 0:00:01\r\u001B[K     |███████▉                        | 368 kB 5.2 MB/s eta 0:00:01\r\u001B[K     |████████                        | 378 kB 5.2 MB/s eta 0:00:01\r\u001B[K     |████████▎                       | 389 kB 5.2 MB/s eta 0:00:01\r\u001B[K     |████████▌                       | 399 kB 5.2 MB/s eta 0:00:01\r\u001B[K     |████████▊                       | 409 kB 5.2 MB/s eta 0:00:01\r\u001B[K     |█████████                       | 419 kB 5.2 MB/s eta 0:00:01\r\u001B[K     |█████████                       | 430 kB 5.2 MB/s eta 0:00:01\r\u001B[K     |█████████▎                      | 440 kB 5.2 MB/s eta 0:00:01\r\u001B[K     |█████████▌                      | 450 kB 5.2 MB/s eta 0:00:01\r\u001B[K     |█████████▊                      | 460 kB 5.2 MB/s eta 0:00:01\r\u001B[K     |██████████                      | 471 kB 5.2 MB/s eta 0:00:01\r\u001B[K     |██████████▏                     | 481 kB 5.2 MB/s eta 0:00:01\r\u001B[K     |██████████▍                     | 491 kB 5.2 MB/s eta 0:00:01\r\u001B[K     |██████████▋                     | 501 kB 5.2 MB/s eta 0:00:01\r\u001B[K     |██████████▉                     | 512 kB 5.2 MB/s eta 0:00:01\r\u001B[K     |███████████                     | 522 kB 5.2 MB/s eta 0:00:01\r\u001B[K     |███████████▎                    | 532 kB 5.2 MB/s eta 0:00:01\r\u001B[K     |███████████▌                    | 542 kB 5.2 MB/s eta 0:00:01\r\u001B[K     |███████████▊                    | 552 kB 5.2 MB/s eta 0:00:01\r\u001B[K     |████████████                    | 563 kB 5.2 MB/s eta 0:00:01\r\u001B[K     |████████████▏                   | 573 kB 5.2 MB/s eta 0:00:01\r\u001B[K     |████████████▍                   | 583 kB 5.2 MB/s eta 0:00:01\r\u001B[K     |████████████▋                   | 593 kB 5.2 MB/s eta 0:00:01\r\u001B[K     |████████████▉                   | 604 kB 5.2 MB/s eta 0:00:01\r\u001B[K     |█████████████                   | 614 kB 5.2 MB/s eta 0:00:01\r\u001B[K     |█████████████▎                  | 624 kB 5.2 MB/s eta 0:00:01\r\u001B[K     |█████████████▌                  | 634 kB 5.2 MB/s eta 0:00:01\r\u001B[K     |█████████████▋                  | 645 kB 5.2 MB/s eta 0:00:01\r\u001B[K     |█████████████▉                  | 655 kB 5.2 MB/s eta 0:00:01\r\u001B[K     |██████████████                  | 665 kB 5.2 MB/s eta 0:00:01\r\u001B[K     |██████████████▎                 | 675 kB 5.2 MB/s eta 0:00:01\r\u001B[K     |██████████████▌                 | 686 kB 5.2 MB/s eta 0:00:01\r\u001B[K     |██████████████▊                 | 696 kB 5.2 MB/s eta 0:00:01\r\u001B[K     |███████████████                 | 706 kB 5.2 MB/s eta 0:00:01\r\u001B[K     |███████████████▏                | 716 kB 5.2 MB/s eta 0:00:01\r\u001B[K     |███████████████▍                | 727 kB 5.2 MB/s eta 0:00:01\r\u001B[K     |███████████████▋                | 737 kB 5.2 MB/s eta 0:00:01\r\u001B[K     |███████████████▉                | 747 kB 5.2 MB/s eta 0:00:01\r\u001B[K     |████████████████                | 757 kB 5.2 MB/s eta 0:00:01\r\u001B[K     |████████████████▎               | 768 kB 5.2 MB/s eta 0:00:01\r\u001B[K     |████████████████▌               | 778 kB 5.2 MB/s eta 0:00:01\r\u001B[K     |████████████████▊               | 788 kB 5.2 MB/s eta 0:00:01\r\u001B[K     |█████████████████               | 798 kB 5.2 MB/s eta 0:00:01\r\u001B[K     |█████████████████▏              | 808 kB 5.2 MB/s eta 0:00:01\r\u001B[K     |█████████████████▍              | 819 kB 5.2 MB/s eta 0:00:01\r\u001B[K     |█████████████████▋              | 829 kB 5.2 MB/s eta 0:00:01\r\u001B[K     |█████████████████▉              | 839 kB 5.2 MB/s eta 0:00:01\r\u001B[K     |██████████████████              | 849 kB 5.2 MB/s eta 0:00:01\r\u001B[K     |██████████████████▏             | 860 kB 5.2 MB/s eta 0:00:01\r\u001B[K     |██████████████████▍             | 870 kB 5.2 MB/s eta 0:00:01\r\u001B[K     |██████████████████▋             | 880 kB 5.2 MB/s eta 0:00:01\r\u001B[K     |██████████████████▉             | 890 kB 5.2 MB/s eta 0:00:01\r\u001B[K     |███████████████████             | 901 kB 5.2 MB/s eta 0:00:01\r\u001B[K     |███████████████████▎            | 911 kB 5.2 MB/s eta 0:00:01\r\u001B[K     |███████████████████▌            | 921 kB 5.2 MB/s eta 0:00:01\r\u001B[K     |███████████████████▊            | 931 kB 5.2 MB/s eta 0:00:01\r\u001B[K     |████████████████████            | 942 kB 5.2 MB/s eta 0:00:01\r\u001B[K     |████████████████████▏           | 952 kB 5.2 MB/s eta 0:00:01\r\u001B[K     |████████████████████▍           | 962 kB 5.2 MB/s eta 0:00:01\r\u001B[K     |████████████████████▋           | 972 kB 5.2 MB/s eta 0:00:01\r\u001B[K     |████████████████████▉           | 983 kB 5.2 MB/s eta 0:00:01\r\u001B[K     |█████████████████████           | 993 kB 5.2 MB/s eta 0:00:01\r\u001B[K     |█████████████████████▎          | 1.0 MB 5.2 MB/s eta 0:00:01\r\u001B[K     |█████████████████████▌          | 1.0 MB 5.2 MB/s eta 0:00:01\r\u001B[K     |█████████████████████▊          | 1.0 MB 5.2 MB/s eta 0:00:01\r\u001B[K     |██████████████████████          | 1.0 MB 5.2 MB/s eta 0:00:01\r\u001B[K     |██████████████████████▏         | 1.0 MB 5.2 MB/s eta 0:00:01\r\u001B[K     |██████████████████████▍         | 1.1 MB 5.2 MB/s eta 0:00:01\r\u001B[K     |██████████████████████▋         | 1.1 MB 5.2 MB/s eta 0:00:01\r\u001B[K     |██████████████████████▊         | 1.1 MB 5.2 MB/s eta 0:00:01\r\u001B[K     |███████████████████████         | 1.1 MB 5.2 MB/s eta 0:00:01\r\u001B[K     |███████████████████████▏        | 1.1 MB 5.2 MB/s eta 0:00:01\r\u001B[K     |███████████████████████▍        | 1.1 MB 5.2 MB/s eta 0:00:01\r\u001B[K     |███████████████████████▋        | 1.1 MB 5.2 MB/s eta 0:00:01\r\u001B[K     |███████████████████████▉        | 1.1 MB 5.2 MB/s eta 0:00:01\r\u001B[K     |████████████████████████        | 1.1 MB 5.2 MB/s eta 0:00:01\r\u001B[K     |████████████████████████▎       | 1.1 MB 5.2 MB/s eta 0:00:01\r\u001B[K     |████████████████████████▌       | 1.2 MB 5.2 MB/s eta 0:00:01\r\u001B[K     |████████████████████████▊       | 1.2 MB 5.2 MB/s eta 0:00:01\r\u001B[K     |█████████████████████████       | 1.2 MB 5.2 MB/s eta 0:00:01\r\u001B[K     |█████████████████████████▏      | 1.2 MB 5.2 MB/s eta 0:00:01\r\u001B[K     |█████████████████████████▍      | 1.2 MB 5.2 MB/s eta 0:00:01\r\u001B[K     |█████████████████████████▋      | 1.2 MB 5.2 MB/s eta 0:00:01\r\u001B[K     |█████████████████████████▉      | 1.2 MB 5.2 MB/s eta 0:00:01\r\u001B[K     |██████████████████████████      | 1.2 MB 5.2 MB/s eta 0:00:01\r\u001B[K     |██████████████████████████▎     | 1.2 MB 5.2 MB/s eta 0:00:01\r\u001B[K     |██████████████████████████▌     | 1.2 MB 5.2 MB/s eta 0:00:01\r\u001B[K     |██████████████████████████▊     | 1.3 MB 5.2 MB/s eta 0:00:01\r\u001B[K     |███████████████████████████     | 1.3 MB 5.2 MB/s eta 0:00:01\r\u001B[K     |███████████████████████████▏    | 1.3 MB 5.2 MB/s eta 0:00:01\r\u001B[K     |███████████████████████████▎    | 1.3 MB 5.2 MB/s eta 0:00:01\r\u001B[K     |███████████████████████████▌    | 1.3 MB 5.2 MB/s eta 0:00:01\r\u001B[K     |███████████████████████████▊    | 1.3 MB 5.2 MB/s eta 0:00:01\r\u001B[K     |████████████████████████████    | 1.3 MB 5.2 MB/s eta 0:00:01\r\u001B[K     |████████████████████████████▏   | 1.3 MB 5.2 MB/s eta 0:00:01\r\u001B[K     |████████████████████████████▍   | 1.3 MB 5.2 MB/s eta 0:00:01\r\u001B[K     |████████████████████████████▋   | 1.4 MB 5.2 MB/s eta 0:00:01\r\u001B[K     |████████████████████████████▉   | 1.4 MB 5.2 MB/s eta 0:00:01\r\u001B[K     |█████████████████████████████   | 1.4 MB 5.2 MB/s eta 0:00:01\r\u001B[K     |█████████████████████████████▎  | 1.4 MB 5.2 MB/s eta 0:00:01\r\u001B[K     |█████████████████████████████▌  | 1.4 MB 5.2 MB/s eta 0:00:01\r\u001B[K     |█████████████████████████████▊  | 1.4 MB 5.2 MB/s eta 0:00:01\r\u001B[K     |██████████████████████████████  | 1.4 MB 5.2 MB/s eta 0:00:01\r\u001B[K     |██████████████████████████████▏ | 1.4 MB 5.2 MB/s eta 0:00:01\r\u001B[K     |██████████████████████████████▍ | 1.4 MB 5.2 MB/s eta 0:00:01\r\u001B[K     |██████████████████████████████▋ | 1.4 MB 5.2 MB/s eta 0:00:01\r\u001B[K     |██████████████████████████████▉ | 1.5 MB 5.2 MB/s eta 0:00:01\r\u001B[K     |███████████████████████████████ | 1.5 MB 5.2 MB/s eta 0:00:01\r\u001B[K     |███████████████████████████████▎| 1.5 MB 5.2 MB/s eta 0:00:01\r\u001B[K     |███████████████████████████████▌| 1.5 MB 5.2 MB/s eta 0:00:01\r\u001B[K     |███████████████████████████████▊| 1.5 MB 5.2 MB/s eta 0:00:01\r\u001B[K     |███████████████████████████████▉| 1.5 MB 5.2 MB/s eta 0:00:01\r\u001B[K     |████████████████████████████████| 1.5 MB 5.2 MB/s \r\n\u001B[?25hCollecting tqdm\r\n  Downloading tqdm-4.65.0-py3-none-any.whl (77 kB)\r\n\u001B[?25l\r\u001B[K     |████▎                           | 10 kB 31.1 MB/s eta 0:00:01\r\u001B[K     |████████▌                       | 20 kB 35.6 MB/s eta 0:00:01\r\u001B[K     |████████████▊                   | 30 kB 41.2 MB/s eta 0:00:01\r\u001B[K     |█████████████████               | 40 kB 45.1 MB/s eta 0:00:01\r\u001B[K     |█████████████████████▎          | 51 kB 45.7 MB/s eta 0:00:01\r\u001B[K     |█████████████████████████▌      | 61 kB 48.5 MB/s eta 0:00:01\r\u001B[K     |█████████████████████████████▊  | 71 kB 49.8 MB/s eta 0:00:01\r\u001B[K     |████████████████████████████████| 77 kB 7.6 MB/s \r\n\u001B[?25hRequirement already satisfied: click in /databricks/python3/lib/python3.9/site-packages (from nltk) (8.0.3)\r\nCollecting regex>=2021.8.3\r\n  Downloading regex-2023.3.23-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (768 kB)\r\n\u001B[?25l\r\u001B[K     |▍                               | 10 kB 6.1 MB/s eta 0:00:01\r\u001B[K     |▉                               | 20 kB 9.6 MB/s eta 0:00:01\r\u001B[K     |█▎                              | 30 kB 10.6 MB/s eta 0:00:01\r\u001B[K     |█▊                              | 40 kB 10.9 MB/s eta 0:00:01\r\u001B[K     |██▏                             | 51 kB 10.4 MB/s eta 0:00:01\r\u001B[K     |██▋                             | 61 kB 11.1 MB/s eta 0:00:01\r\u001B[K     |███                             | 71 kB 11.1 MB/s eta 0:00:01\r\u001B[K     |███▍                            | 81 kB 10.8 MB/s eta 0:00:01\r\u001B[K     |███▉                            | 92 kB 11.8 MB/s eta 0:00:01\r\u001B[K     |████▎                           | 102 kB 11.4 MB/s eta 0:00:01\r\u001B[K     |████▊                           | 112 kB 11.4 MB/s eta 0:00:01\r\u001B[K     |█████▏                          | 122 kB 11.4 MB/s eta 0:00:01\r\u001B[K     |█████▌                          | 133 kB 11.4 MB/s eta 0:00:01\r\u001B[K     |██████                          | 143 kB 11.4 MB/s eta 0:00:01\r\u001B[K     |██████▍                         | 153 kB 11.4 MB/s eta 0:00:01\r\u001B[K     |██████▉                         | 163 kB 11.4 MB/s eta 0:00:01\r\u001B[K     |███████▎                        | 174 kB 11.4 MB/s eta 0:00:01\r\u001B[K     |███████▊                        | 184 kB 11.4 MB/s eta 0:00:01\r\u001B[K     |████████                        | 194 kB 11.4 MB/s eta 0:00:01\r\u001B[K     |████████▌                       | 204 kB 11.4 MB/s eta 0:00:01\r\u001B[K     |█████████                       | 215 kB 11.4 MB/s eta 0:00:01\r\u001B[K     |█████████▍                      | 225 kB 11.4 MB/s eta 0:00:01\r\u001B[K     |█████████▉                      | 235 kB 11.4 MB/s eta 0:00:01\r\u001B[K     |██████████▎                     | 245 kB 11.4 MB/s eta 0:00:01\r\u001B[K     |██████████▋                     | 256 kB 11.4 MB/s eta 0:00:01\r\u001B[K     |███████████                     | 266 kB 11.4 MB/s eta 0:00:01\r\u001B[K     |███████████▌                    | 276 kB 11.4 MB/s eta 0:00:01\r\u001B[K     |████████████                    | 286 kB 11.4 MB/s eta 0:00:01\r\u001B[K     |████████████▍                   | 296 kB 11.4 MB/s eta 0:00:01\r\u001B[K     |████████████▉                   | 307 kB 11.4 MB/s eta 0:00:01\r\u001B[K     |█████████████▏                  | 317 kB 11.4 MB/s eta 0:00:01\r\u001B[K     |█████████████▋                  | 327 kB 11.4 MB/s eta 0:00:01\r\u001B[K     |██████████████                  | 337 kB 11.4 MB/s eta 0:00:01\r\u001B[K     |██████████████▌                 | 348 kB 11.4 MB/s eta 0:00:01\r\u001B[K     |███████████████                 | 358 kB 11.4 MB/s eta 0:00:01\r\u001B[K     |███████████████▍                | 368 kB 11.4 MB/s eta 0:00:01\r\u001B[K     |███████████████▊                | 378 kB 11.4 MB/s eta 0:00:01\r\u001B[K     |████████████████▏               | 389 kB 11.4 MB/s eta 0:00:01\r\u001B[K     |████████████████▋               | 399 kB 11.4 MB/s eta 0:00:01\r\u001B[K     |█████████████████               | 409 kB 11.4 MB/s eta 0:00:01\r\u001B[K     |█████████████████▌              | 419 kB 11.4 MB/s eta 0:00:01\r\u001B[K     |██████████████████              | 430 kB 11.4 MB/s eta 0:00:01\r\u001B[K     |██████████████████▎             | 440 kB 11.4 MB/s eta 0:00:01\r\u001B[K     |██████████████████▊             | 450 kB 11.4 MB/s eta 0:00:01\r\u001B[K     |███████████████████▏            | 460 kB 11.4 MB/s eta 0:00:01\r\u001B[K     |███████████████████▋            | 471 kB 11.4 MB/s eta 0:00:01\r\u001B[K     |████████████████████            | 481 kB 11.4 MB/s eta 0:00:01\r\u001B[K     |████████████████████▌           | 491 kB 11.4 MB/s eta 0:00:01\r\u001B[K     |████████████████████▉           | 501 kB 11.4 MB/s eta 0:00:01\r\u001B[K     |█████████████████████▎          | 512 kB 11.4 MB/s eta 0:00:01\r\u001B[K     |█████████████████████▊          | 522 kB 11.4 MB/s eta 0:00:01\r\u001B[K     |██████████████████████▏         | 532 kB 11.4 MB/s eta 0:00:01\r\u001B[K     |██████████████████████▋         | 542 kB 11.4 MB/s eta 0:00:01\r\u001B[K     |███████████████████████         | 552 kB 11.4 MB/s eta 0:00:01\r\u001B[K     |███████████████████████▍        | 563 kB 11.4 MB/s eta 0:00:01\r\u001B[K     |███████████████████████▉        | 573 kB 11.4 MB/s eta 0:00:01\r\u001B[K     |████████████████████████▎       | 583 kB 11.4 MB/s eta 0:00:01\r\u001B[K     |████████████████████████▊       | 593 kB 11.4 MB/s eta 0:00:01\r\u001B[K     |█████████████████████████▏      | 604 kB 11.4 MB/s eta 0:00:01\r\u001B[K     |█████████████████████████▋      | 614 kB 11.4 MB/s eta 0:00:01\r\u001B[K     |██████████████████████████      | 624 kB 11.4 MB/s eta 0:00:01\r\u001B[K     |██████████████████████████▍     | 634 kB 11.4 MB/s eta 0:00:01\r\u001B[K     |██████████████████████████▉     | 645 kB 11.4 MB/s eta 0:00:01\r\u001B[K     |███████████████████████████▎    | 655 kB 11.4 MB/s eta 0:00:01\r\u001B[K     |███████████████████████████▊    | 665 kB 11.4 MB/s eta 0:00:01\r\u001B[K     |████████████████████████████▏   | 675 kB 11.4 MB/s eta 0:00:01\r\u001B[K     |████████████████████████████▌   | 686 kB 11.4 MB/s eta 0:00:01\r\u001B[K     |█████████████████████████████   | 696 kB 11.4 MB/s eta 0:00:01\r\u001B[K     |█████████████████████████████▍  | 706 kB 11.4 MB/s eta 0:00:01\r\u001B[K     |█████████████████████████████▉  | 716 kB 11.4 MB/s eta 0:00:01\r\u001B[K     |██████████████████████████████▎ | 727 kB 11.4 MB/s eta 0:00:01\r\u001B[K     |██████████████████████████████▊ | 737 kB 11.4 MB/s eta 0:00:01\r\u001B[K     |███████████████████████████████ | 747 kB 11.4 MB/s eta 0:00:01\r\u001B[K     |███████████████████████████████▌| 757 kB 11.4 MB/s eta 0:00:01\r\u001B[K     |████████████████████████████████| 768 kB 11.4 MB/s eta 0:00:01\r\u001B[K     |████████████████████████████████| 768 kB 11.4 MB/s \r\n\u001B[?25hRequirement already satisfied: joblib in /databricks/python3/lib/python3.9/site-packages (from nltk) (1.0.1)\r\nInstalling collected packages: tqdm, regex, nltk\r\nSuccessfully installed nltk-3.8.1 regex-2023.3.23 tqdm-4.65.0\r\n\u001B[33mWARNING: You are using pip version 21.2.4; however, version 23.1 is available.\r\nYou should consider upgrading via the '/local_disk0/.ephemeral_nfs/envs/pythonEnv-3ba20400-e4c0-42cb-9e1a-e69a506766ff/bin/python -m pip install --upgrade pip' command.\u001B[0m\r\n"]}],"execution_count":0},{"cell_type":"code","source":["!pip install beautifulsoup4"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"3e80eda5-07bc-46b3-a457-44e124c62ec4","inputWidgets":{},"title":""}},"outputs":[{"output_type":"stream","output_type":"stream","name":"stdout","text":["Collecting beautifulsoup4\r\n  Downloading beautifulsoup4-4.12.2-py3-none-any.whl (142 kB)\r\n\u001B[?25l\r\u001B[K     |██▎                             | 10 kB 20.6 MB/s eta 0:00:01\r\u001B[K     |████▋                           | 20 kB 4.3 MB/s eta 0:00:01\r\u001B[K     |██████▉                         | 30 kB 6.1 MB/s eta 0:00:01\r\u001B[K     |█████████▏                      | 40 kB 4.9 MB/s eta 0:00:01\r\u001B[K     |███████████▌                    | 51 kB 4.9 MB/s eta 0:00:01\r\u001B[K     |█████████████▊                  | 61 kB 5.7 MB/s eta 0:00:01\r\u001B[K     |████████████████                | 71 kB 5.7 MB/s eta 0:00:01\r\u001B[K     |██████████████████▍             | 81 kB 5.7 MB/s eta 0:00:01\r\u001B[K     |████████████████████▋           | 92 kB 6.3 MB/s eta 0:00:01\r\u001B[K     |███████████████████████         | 102 kB 5.7 MB/s eta 0:00:01\r\u001B[K     |█████████████████████████▏      | 112 kB 5.7 MB/s eta 0:00:01\r\u001B[K     |███████████████████████████▌    | 122 kB 5.7 MB/s eta 0:00:01\r\u001B[K     |█████████████████████████████▉  | 133 kB 5.7 MB/s eta 0:00:01\r\u001B[K     |████████████████████████████████| 142 kB 5.7 MB/s \r\n\u001B[?25hCollecting soupsieve>1.2\r\n  Downloading soupsieve-2.4.1-py3-none-any.whl (36 kB)\r\nInstalling collected packages: soupsieve, beautifulsoup4\r\nSuccessfully installed beautifulsoup4-4.12.2 soupsieve-2.4.1\r\n\u001B[33mWARNING: You are using pip version 21.2.4; however, version 23.1 is available.\r\nYou should consider upgrading via the '/local_disk0/.ephemeral_nfs/envs/pythonEnv-3ba20400-e4c0-42cb-9e1a-e69a506766ff/bin/python -m pip install --upgrade pip' command.\u001B[0m\r\n"]}],"execution_count":0},{"cell_type":"code","source":["#doanloading nltk library\nimport nltk\nnltk.download('stopwords')\nnltk.download('punkt')"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"d779aaa8-d98f-4518-8633-317536763b67","inputWidgets":{},"title":""}},"outputs":[{"output_type":"stream","output_type":"stream","name":"stdout","text":["[nltk_data] Downloading package stopwords to /root/nltk_data...\n[nltk_data]   Unzipping corpora/stopwords.zip.\n[nltk_data] Downloading package punkt to /root/nltk_data...\n[nltk_data]   Unzipping tokenizers/punkt.zip.\nOut[3]: True"]}],"execution_count":0},{"cell_type":"code","source":["from nltk.stem import WordNetLemmatizer\nnltk.download('wordnet')\nnltk.download('omw-1.4')"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"dce17636-f467-416e-add2-82f6a7c18931","inputWidgets":{},"title":""}},"outputs":[{"output_type":"stream","output_type":"stream","name":"stdout","text":["[nltk_data] Downloading package wordnet to /root/nltk_data...\n[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\nOut[4]: True"]}],"execution_count":0},{"cell_type":"code","source":["from pyspark.sql.functions import sum, regexp_replace, col, explode, desc, split, year, lit\nfrom pyspark.sql.types import IntegerType, FloatType, DataType, TimestampType, StringType, StructType, StructField, ArrayType\nimport matplotlib.pyplot as plt\nfrom nltk.corpus import stopwords\nfrom nltk.tokenize import sent_tokenize, word_tokenize\nfrom bs4 import BeautifulSoup\nimport string\nimport re\nimport matplotlib.pyplot as plt\nfrom collections import defaultdict\nimport math\n"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"c9a20e60-c0cf-4fad-8868-d45013740229","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["#### Importing files into dataframes"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"a37d9cbc-b403-4c31-abf6-0e7ce478070d","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["from pyspark.sql.functions import sum\nfrom pyspark.sql.types import IntegerType, FloatType, DataType, TimestampType, StringType, StructType, StructField\n\n\n# File location\nfilename1 = \"/FileStore/tables/Assignment1/SO_ML.csv\"\nfilename2 = \"/FileStore/tables/Assignment1/SO_Spark.csv\"\nfilename3 = \"/FileStore/tables/Assignment1/SO_Security.csv\"\n\n#Function to import csv files\ndef import_csv(filename):\n     #creating schema to ensure files are in appropriate format.\n    new_Schema = StructType([ StructField(\"Id\", IntegerType(), True),\n                      StructField(\"PostTypeId\", IntegerType(), True),\n                      StructField(\"AcceptedAnswerId\", IntegerType(), True),\n                      StructField(\"ParentId\", IntegerType(), True),\n                       StructField(\"CreationDate\", TimestampType(), True),\n                       StructField(\"DeletionDat\", TimestampType(), True),\n                       StructField(\"Score\", IntegerType(), True),\n                       StructField(\"ViewCount\", IntegerType(), True),\n                       StructField(\"Body\", StringType(), True),\n                       StructField(\"OwnerUserId\", IntegerType(), True),\n                       StructField(\"OwnerDisplayName\", StringType(), True),\n                       StructField(\"LastEditorUserId\", IntegerType(), True),\n                       StructField(\"LastEditorDisplayName\", StringType(), True),\n                       StructField(\"LastEditDate\", TimestampType(), True),\n                       StructField(\"LastActivityDate\", TimestampType(), True),\n                       StructField(\"Title\", StringType(), True),\n                       StructField(\"Tags\", StringType(), True),\n                       StructField(\"AnswerCount\", IntegerType(), True),\n                       StructField(\"CommentCount\", IntegerType(), True),\n                       StructField(\"FavoriteCount\", IntegerType(), True),\n                       StructField(\"ClosedDate\", TimestampType(), True),\n                       StructField(\"CommunityOwnedDate\", TimestampType(), True),\n                       StructField(\"ContentLicense\", StringType(), True)\n                      ])\n\n\n    # CSV options\n    infer_schema = \"false\"\n    first_row_is_header = \"true\"\n    delimiter = \",\"\n    multiLine = \"true\"\n    escape = \"\\\"\"\n\n    # CSV options\n    file_type = \"csv\"\n    infer_schema = \"true\"\n    first_row_is_header = \"true\"\n    delimiter = \",\"\n    \n    #loading file\n    df = spark.read.format(file_type) \\\n  .option(\"inferSchema\", infer_schema) \\\n  .option(\"header\", first_row_is_header) \\\n  .option(\"sep\", delimiter) \\\n  .option(\"multiLine\", multiLine) \\\n  .option(\"escape\", escape) \\\n  .schema(new_Schema) \\\n  .load(\"/FileStore/tables/Assignment1/\"+filename)\n    \n    return df    \n\n#function to perform column arithmetic (row count and sum of column values)\ndef column_sum(df, col_name):\n    #Using count() to count the number of rows\n    row_count = df.count()\n    print(\"The total number of ViewCount in the file is: \" + str(row_count))\n    #Using sum() to calculate the sum of the ViewCounts and displaying it.\n    df.select(sum(col_name)).show()\n\n#assigning the names of the files into variables\nfile_ML = \"SO_ML.csv\"\nfile_Spark = \"SO_Spark.csv\"\nfile_Security = \"SO_Security.csv\"\n\n#importing each file\ndf_ML = import_csv(file_ML)\ndf_Spark = import_csv(file_Spark)\ndf_Security = import_csv(file_Security)"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"6e526a73-6314-47ed-9fa6-79b299047df6","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["#function to perform column arithmetic (row count and sum of column values)\ndef column_sum(df, col_name):\n    #Using count() to count the number of rows\n    row_count = df.count()\n    print(\"The total number of ViewCount in the file is: \" + str(row_count))\n    #Using sum() to calculate the sum of the ViewCounts and displaying it.\n    df.select(sum(col_name)).show()\n\nprint(file_ML)\ncolumn_sum(df_ML, \"ViewCount\")\n\nprint(file_Spark)\ncolumn_sum(df_Spark, \"ViewCount\")\n\nprint(file_Security)\ncolumn_sum(df_Security, \"ViewCount\")"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"7c88f1e2-3da9-4439-8b75-926cb71e036b","inputWidgets":{},"title":""}},"outputs":[{"output_type":"stream","output_type":"stream","name":"stdout","text":["SO_ML.csv\nThe total number of ViewCount in the file is: 50000\n+--------------+\n|sum(ViewCount)|\n+--------------+\n|     102120351|\n+--------------+\n\nSO_Spark.csv\nThe total number of ViewCount in the file is: 50000\n+--------------+\n|sum(ViewCount)|\n+--------------+\n|     140958653|\n+--------------+\n\nSO_Security.csv\nThe total number of ViewCount in the file is: 50000\n+--------------+\n|sum(ViewCount)|\n+--------------+\n|     163597256|\n+--------------+\n\n"]}],"execution_count":0},{"cell_type":"markdown","source":["### Preprocessing of data in Files"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"e6a490c1-c898-457a-9bcd-0e01c794610f","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["#Merging columns\n@udf\ndef column_merger(col1, col2):\n    newCol= col1 + \" \" + col2\n    return newCol\n    \ndf_ML_Combined = df_ML.select(\"*\",column_merger(\"Title\",\"Body\").alias(\"TitleBody\"))\ndf_Spark_Combined = df_Spark.select(\"*\",column_merger(\"Title\",\"Body\").alias(\"TitleBody\"))\ndf_Security_Combined = df_Security.select(\"*\",column_merger(\"Title\",\"Body\").alias(\"TitleBody\"))\n#display(df_ML_Combined)"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"83123d54-3e21-4188-aba5-7fbb0858a7bd","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["#parsing html text in words\n@udf\ndef preProcessText(body):\n    newBody=\"\"\n    #soups = BeautifulSoup(body, \"html.parser\").find_all(\"p\")\n    soups = BeautifulSoup(body, \"html.parser\")\n    \n    #code tag is removed\n    for data in soups(['code']):\n        data.decompose()\n    \n    #content of remainder tags extracted\n    for d in soups:\n        newBody+=d.get_text().lower()\n    return newBody\n    \n\n#Parsing all data files\nnewML = df_ML_Combined.select(\"*\",preProcessText(\"TitleBody\").alias(\"newBody\"))\nnewSpark = df_Spark_Combined.select(\"*\",preProcessText(\"TitleBody\").alias(\"newBody\"))\nnewSecurity = df_Security_Combined.select(\"*\",preProcessText(\"TitleBody\").alias(\"newBody\"))\n#display(newML)"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"ea5a4cb4-cced-4677-ac1d-275369f1a490","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["#Function to remove stopwords and lemmatization\n@udf\ndef removeStopWords(sentences):\n    lemmatizer = WordNetLemmatizer()\n    #stemmer = nltk.stem.SnowballStemmer()\n    noStopWords = []\n    stopword = stopwords.words('english')\n    for sentence in sent_tokenize(sentences): #breaking into sentences\n        for word in word_tokenize(sentence): #breaking into words\n            if word not in stopword and len(word)>2: #removing stopwords and other irrelevant words\n                if \"n't\" not in word: #because words like \"doest\" appear before doesn't leaving alot of \"n't\"\n                    noStopWords.append(lemmatizer.lemmatize(word))\n                    #noStopWords.append(stemmer.stem(word))\n    return ' '.join(noStopWords)\n\n#Removing stopwords from files\nnewMLnonstop1 = newML.select(\"*\", removeStopWords(\"newBody\").alias(\"noStopWords\"))\nnewSparknonstop1 = newSpark.select(\"*\", removeStopWords(\"newBody\").alias(\"noStopWords\"))\nnewSecuritynonstop1 = newSecurity.select(\"*\", removeStopWords(\"newBody\").alias(\"noStopWords\"))"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"eb448d01-f1e5-428a-9860-cdad3fd309a2","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["#UDF to remove puntuations and non-alphabets\n@udf\ndef removePuncNonalphabets(column):\n    \n    #using regex to extract only alphabets and whitespaces\n    return re.sub('[^a-zA-Z\\s]+', \"\",column)\n    \n#removing punctuation and non-alphabets\nfiltered_ML = newMLnonstop1.withColumn(\"cleanWords\", removePuncNonalphabets(\"noStopWords\"))\nfiltered_Spark = newSparknonstop1.withColumn(\"cleanWords\", removePuncNonalphabets(\"noStopWords\"))\nfiltered_Security = newSecuritynonstop1.withColumn(\"cleanWords\", removePuncNonalphabets(\"noStopWords\"))"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"37fc7021-6c17-41a1-bb6c-afb791684c29","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["#getting most unanswered questions\ndef maxUnansweredQuestion(df):\n    #Extracting unaswered questions\n    unanswered = df.filter(df['AcceptedAnswerId'].isNull())\n    sortedUnanswered = unanswered.sort(\"Score\", ascending=False)\n    return sortedUnanswered.first(), sortedUnanswered\n\n#\nmaxUnansweredML, unanswered_ML = maxUnansweredQuestion(filtered_ML)\nmaxUnansweredSpark, unanswered_Spark = maxUnansweredQuestion(filtered_Spark)\nmaxUnansweredSecurity, unanswered_Security = maxUnansweredQuestion(filtered_Security)"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"c4bbc706-29b8-4b4f-933b-e2c613617784","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["#display(unanswered_Spark)"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"c895cc9f-2a7f-4df4-92b3-4fa41044c691","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["print(\"Max Score in ML\")\nprint(maxUnansweredML['cleanWords'])\nprint(\"\\nMax Score in Spark\")\n\nprint(maxUnansweredSpark['cleanWords'])\n\nprint(\"\\nMax Score in Security\")\nprint(maxUnansweredSecurity['cleanWords'])"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"dcfd17dd-762a-404e-97ae-50796866cf28","inputWidgets":{},"title":""}},"outputs":[{"output_type":"stream","output_type":"stream","name":"stdout","text":["Max Score in ML\nepoch iteration training neural network difference epoch iteration training multilayer perceptron\n\nMax Score in Spark\nspark javalangoutofmemoryerror java heap space cluster master slave node memory setting problem first read data  hdfs rdd second something rdd last output hdfs run program show many task every thing input data  solve problem\n\nMax Score in Security\ntokenbased authentication want understand tokenbased authentication mean searched internet could find anything understandable\n"]}],"execution_count":0},{"cell_type":"code","source":["#UDF to calculate cosine similarity\n@udf\ndef cosineSimilarity(posAnswer, unaccepted):\n    #combining unanswered question and possible similar question\n    dwords = list(set(unaccepted.split() + posAnswer.split()))\n    \n    dt1 = []\n    dt2 = []\n    \n    for word in dwords:\n        if word in posAnswer:\n            dt1.append(1)\n        else:\n            dt1.append(0)\n    \n        if word in maxUn:\n            dt2.append(1)\n        else:\n            dt2.append(0)\n    \n    #counter = 0\n    dotProduct = 0\n    t1Summer = 0\n    t2Summer = 0\n    \n    for i in range(0,len(dt1)):\n    #while(counter< len(dt1)):\n        dotProduct += dt1[i]*dt2[i]\n        t1Summer += dt1[i]*dt1[i]\n        t2Summer += dt2[i]*dt2[i]\n        #counter+=1\n    \n    root = math.sqrt(t1Summer*t2Summer)\n    \n    if root ==0:\n        return 0\n    \n    return (dotProduct/root)\n\n\nsimilarity_ML = filtered_ML.filter(filtered_ML[\"AcceptedAnswerId\"].isNotNull()).withColumn(\"similarity\",cosineSimilarity(col(\"cleanWords\"),lit(maxUnansweredML['cleanWords']))).sort(\"similarity\",ascending=False)\n\nsimilarity_Spark = filtered_Spark.filter(filtered_Spark[\"AcceptedAnswerId\"].isNotNull()).withColumn(\"similarity\",cosineSimilarity(col(\"cleanWords\"),lit(maxUnansweredSpark['cleanWords']))).sort(\"similarity\",ascending=False)\n\nsimilarity_Security = filtered_Security.filter(filtered_Security[\"AcceptedAnswerId\"].isNotNull()).withColumn(\"similarity\",cosineSimilarity(col(\"cleanWords\"),lit(maxUnansweredSecurity['cleanWords']))).sort(\"similarity\",ascending=False)  "],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"5885051f-9902-423b-90cc-fea7fdb8d500","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["#display(similarity_ML)"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"efefe59e-e3e6-43f4-9692-762d04f1fd3b","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["#display(similarity_Spark)"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"7dd48aeb-ccb1-424f-b914-87e988191b12","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["#display(similarity_Security)"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"4bc1ba5d-1a3d-43bf-be66-1899b8fc34a2","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["#Selecting the top 3 rows with the most similar questions\nTop3_ML = similarity_ML.take(3)\nTop3_Spark = similarity_Spark.take(3)\nTop3_Security = similarity_Security.take(3)"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"f3a33452-1f7f-4e9a-adab-dd1c4c7ce4b9","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["print(\"**********************ML****************************\")\nprint(\"The unanswered question with the highest score in ML: \")\nprint(\"Id = {} \\n {}\".format(maxUnansweredML['Id'], maxUnansweredML['newBody']))\nprint(\"The top 3 most similar questions with accepted answers:\")\nprint(\"1. Id = {} \\nSimilarity Score: {}\\nQuestion: {} \\n\".format(Top3_ML[0]['Id'],Top3_ML[0]['similarity'], Top3_ML[0]['newBody'] ))\nprint(\"2. Id = {} \\nSimilarity Score: {}\\nQuestion: {} \\n\".format(Top3_ML[1]['Id'],Top3_ML[1]['similarity'], Top3_ML[1]['newBody']))\nprint(\"3. Id = {} \\nSimilarity Score: {}\\nQuestion: {} \\n\".format(Top3_ML[2]['Id'],Top3_ML[2]['similarity'], Top3_ML[2]['newBody']))"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"54b88e65-a7dd-40a8-bcd8-7d8a87285c0d","inputWidgets":{},"title":""}},"outputs":[{"output_type":"stream","output_type":"stream","name":"stdout","text":["**********************ML****************************\nThe unanswered question with the highest score in ML: \nId = 4752626 \n epoch vs iteration when training neural networks what is the difference between epoch and iteration when training a multi-layer perceptron?\n\nThe top 3 most similar questions with accepted answers:\n1. Id = 46621774 \nSimilarity Score: 0.5892556509887896\nQuestion: epochs vs pass vs iteration what does the term epochs mean in the neural network.\nhow does it differ from pass and iteration\n \n\n2. Id = 60691599 \nSimilarity Score: 0.4472135954999579\nQuestion: sudden drop in validation accuracy during training when i was training my neural network there is a sudden drop in validation accuracy during the 8th epoch what does this mean?\n\n \n\n3. Id = 69808910 \nSimilarity Score: 0.4472135954999579\nQuestion: why does the result from evaluate() differ from last epoch result? i have a very simple neural-network which works in 250 epochs and in the last epoch it shows the  , however, if i try to get the  then the mae is about .\nwhy there is such big difference ?\nhere is my code:\n\n \n\n"]}],"execution_count":0},{"cell_type":"code","source":["print(\"**********************SPARK****************************\")\nprint(\"The unanswered question with the highest score in Spark: \")\nprint(\"Id = {} \\n{}\".format(maxUnansweredSpark['Id'], maxUnansweredSpark['newBody']))\nprint(\"\\n\\nThe top 3 most similar questions with accepted answers:\\n\")\nprint(\"1. Id = {} \\nSimilarity Score: {} \\n{}\\n\".format(Top3_Spark[0]['Id'],Top3_Spark[0]['similarity'], Top3_Spark[0]['newBody']))\nprint(\"2. Id = {} \\nSimilarity Score: {} \\n{}\\n\".format(Top3_Spark[1]['Id'], Top3_Spark[1]['similarity'], Top3_Spark[1]['newBody']))\nprint(\"3. Id = {} \\nSimilarity Score: {} \\n{}\\n\".format(Top3_Spark[2]['Id'], Top3_Spark[2]['similarity'], Top3_Spark[2]['newBody']))"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"fc387da9-f8e1-47e1-964b-2cc20979fbe9","inputWidgets":{},"title":""}},"outputs":[{"output_type":"stream","output_type":"stream","name":"stdout","text":["**********************SPARK****************************\nThe unanswered question with the highest score in Spark: \nId = 21138751 \nspark java.lang.outofmemoryerror: java heap space my cluster: 1 master, 11 slaves, each node has 6 gb memory.\nmy settings:\n\nhere is the problem:\nfirst, i read some data (2.19 gb) from hdfs to rdd:\n\nsecond, do something on this rdd:\n\nlast, output to hdfs:\n\nwhen i run my program it shows:\n\nthere are too many tasks?\nps: every thing is ok when the input data is about 225 mb. \nhow can i solve this problem?\n\n\n\nThe top 3 most similar questions with accepted answers:\n\n1. Id = 49304799 \nSimilarity Score: 0.4117055577420104 \njava spark collect() javardd fails with memory errors (emr cluster) im running spark app on aws emr(elastic map reduce) cluster\nmy master node characteristics are: 8 vcore, 15 gib memory, 80 ssd gb storage\nmy executors node are: 8 vcore, 15 gib memory, 80 ssd gb storage\ni have csv inputfile  file with size - 600mb.i am trying to read it to javardd and then use collect() to convert it to list of objects.\nhere is my code:\n\neverytime i try to run this code, i get  java.lang.outofmemoryerror: java heap space. \nas far as i understand spark is performing collect() operation on my master node. so is there any way to increase the memory, so it would be able to run the program? \n\n\n\n2. Id = 32206785 \nSimilarity Score: 0.3799802978286742 \nspark job running out of heap memory on takesample i've got an apache spark cluster with one master node and three worker nodes. the worker nodes have 32 cores and 124g of memory each.  i've also got a dataset in hdfs with around 650 million text records.  this dataset is a number of serialized rdds read in like so:\n\ni'd like to extract a sample of one million of these records to do some analytics, so i figured i'd try .  however, that eventually fails with this error message:\n\ni understand that i'm running out of heap space (on the driver, i think?), and that makes sense.  doing , the dataset takes up 2575 gigabytes on disk (but is only ~850 gb in size).\nso, my question is, what can i do to extract this sample of 1000000 records (which i later plan on serializing to disk)?  i know i could just do  with smaller sample sizes and aggregate them later, but i think i'm just not setting the correct configuration or doing something wrong, which is preventing me from doing this the way i'd like.\n\n\n3. Id = 40764412 \nSimilarity Score: 0.37716825457067865 \nwhy does spark streaming fail at string decoding due to java.lang.outofmemoryerror? i run a spark streaming ( api) application on a yarn cluster of 3 nodes with 128g ram each (!) the app reads records from a kafka topic and writes to hdfs.\nmost of the time the application fails/is killed (mostly receiver fails) due to java heap error no matter how much memory i configure to executor/driver.\n\n\n\n"]}],"execution_count":0},{"cell_type":"code","source":["print(\"******************SECURITY**************************\")\nprint(\"The unanswered question with the highest score in Security: \")\nprint(\"Id = {} \\n{}\".format(maxUnansweredSecurity['Id'], maxUnansweredSecurity['cleanWords']))\nprint(\"\\n\\nThe top 3 most similar questions with accepted answers:\\n\")\nprint(\"1. Id = {} \\nSimilarity Score: {} \\n{}\\n\".format(Top3_Security[0]['Id'], Top3_Security[0]['similarity'],Top3_Security[0]['newBody']))\nprint(\"2. Id = {} \\nSimilarity Score: {} \\n{}\\n\".format(Top3_Security[1]['Id'], Top3_Security[1]['similarity'], Top3_Security[1]['newBody']))\nprint(\"3. Id = {} \\nSimilarity Score: {} \\n{}\\n\".format(Top3_Security[2]['Id'], Top3_Security[2]['similarity'], Top3_Security[2]['newBody']))"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"d34ebe95-c712-4e52-8f57-55a6295ab685","inputWidgets":{},"title":""}},"outputs":[{"output_type":"stream","output_type":"stream","name":"stdout","text":["******************SECURITY**************************\nThe unanswered question with the highest score in Security: \nId = 1592534 \ntokenbased authentication want understand tokenbased authentication mean searched internet could find anything understandable\n\n\nThe top 3 most similar questions with accepted answers:\n\n1. Id = 62015970 \nSimilarity Score: 0.4082482904638631 \nwhat does -nd argument of ssh mean? what does the -nd argument of ssh mean? i didn't manage to find it on the internet.\nexample:\n\n\n\n2. Id = 37314142 \nSimilarity Score: 0.36563621206356534 \nsecurity mechanism in wsdl i want to know how wsdl secure data on trafic. i searched but i can't find anything i need. this is a wsdl service sample and i want to understand their security mechanism.\n\nthis \n\nis encode data trafic right ?\n\n\n3. Id = 4750377 \nSimilarity Score: 0.3481553119113957 \nauthentication by delegation: what does it mean? what does \"authentication by delegation\" mean?\n\n\n"]}],"execution_count":0},{"cell_type":"code","source":[""],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"9937caf2-7ca0-47c6-bbfe-41d589c4d3db","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0}],"metadata":{"application/vnd.databricks.v1+notebook":{"notebookName":"StackOverflow_NLP","dashboards":[],"notebookMetadata":{"pythonIndentUnit":4},"language":"python","widgets":{},"notebookOrigID":110136427625572}},"nbformat":4,"nbformat_minor":0}
